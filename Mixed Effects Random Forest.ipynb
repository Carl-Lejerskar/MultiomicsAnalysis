{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Effects Random Forest - Immune System Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from merf.merf import MERF\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "def load_obj(name ):                    \n",
    "    with open('' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the study the authors employed a LOPOCV methodology to address the fact that samples collected for each subject are dependent on their respective subject.\n",
    "\n",
    "Another approach would be acknowledging the clustered nature of this data, and employ a suitable model where observations are dependent on the patient. Below I will implement a Mixed Effects Random Forest algorithm, clustered by patient.\n",
    "\n",
    "Due to computational limits (this MERF package is quite intensive) I will only be utilizing the immune system dataset, and there will be extreme limitations put in place on cross validation as this package is not compatible with the more efficient GridSearchCV()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets['immunesystem']\n",
    "df = df[['_BL' not in x for x in df.index]] #fixing indices\n",
    "df['response'] = datasets['response']\n",
    "df.rename(columns={'Unnamed: 0':'PatientID'},inplace=True)\n",
    "df['patient'] = list(range(1,18))*3\n",
    "df['Z'] = np.ones(df.shape[0])\n",
    "X = df.drop('response', axis=1)\n",
    "y = df['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Hyperparameters - Nested Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the top layer test/train split. The testing set will be left aside for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_meta, X_test_meta, y_train_meta, y_test_meta = train_test_split(X, y, test_size=0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I split the training set to for hyperparameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-0b4d64409172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mbest_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE: %0.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Grid:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_score' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"merf\").setLevel(logging.WARNING)\n",
    "grid = {\n",
    "            \"n_estimators\" : [100,200,300,400],\n",
    "            \"max_iterations\" : [100,150],\n",
    "        }\n",
    "mselist = []\n",
    "grids = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_meta, y_train_meta, test_size=0.33, random_state = 42)\n",
    "for g in ParameterGrid(grid):\n",
    "        train_clusters = X_train['patient']\n",
    "        z_train = X_train[['Z']]\n",
    "        X_train = X_train.drop(['patient','Z'], axis=1)\n",
    "        z_test = X_test[['Z']]\n",
    "        test_clusters = X_test['patient']\n",
    "        X_test = X_test.drop(['patient','Z'], axis=1)\n",
    "        mrf = MERF(**g)\n",
    "        mrf.fit(X_train, z_train, train_clusters, y_train)\n",
    "        predictions = mrf.predict(X_test, z_test, test_clusters)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mselist.append(mse)\n",
    "        grids.append(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above was due to not saving the best score/grid correctly. Instead of re-running this code, I can simply observe the best values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'max_iterations': 100, 'n_estimators': 100},\n",
       " {'max_iterations': 100, 'n_estimators': 200},\n",
       " {'max_iterations': 100, 'n_estimators': 300},\n",
       " {'max_iterations': 100, 'n_estimators': 400},\n",
       " {'max_iterations': 150, 'n_estimators': 100},\n",
       " {'max_iterations': 150, 'n_estimators': 200},\n",
       " {'max_iterations': 150, 'n_estimators': 300},\n",
       " {'max_iterations': 150, 'n_estimators': 400}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.962749552396417,\n",
       " 18.05286336323495,\n",
       " 18.93322877699514,\n",
       " 19.02728452950163,\n",
       " 20.69524972008702,\n",
       " 17.741120573519115,\n",
       " 18.820842411308202,\n",
       " 20.475879801385684]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mselist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking above, the MSE's look comparable. Due to computational limitations I will select the first grid corresponding to a MSE of ~17.96 as it will reduce runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_parameters = {'max_iterations': 100, 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I fit the model on the entirety of the training set, and evaluate its performance on the test set that was left aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<merf.merf.MERF at 0x12891ab70>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clusters = X_train_meta['patient']\n",
    "z_train = X_train_meta[['Z']]\n",
    "X_train_meta = X_train_meta.drop(['patient','Z'], axis=1)\n",
    "z_test = X_test_meta[['Z']]\n",
    "test_clusters = X_test_meta['patient']\n",
    "X_test_meta = X_test_meta.drop(['patient','Z'], axis=1)\n",
    "mrf = MERF(**selected_parameters)\n",
    "mrf.fit(X_train_meta, z_train, train_clusters, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.234772335300683\n"
     ]
    }
   ],
   "source": [
    "predictions = mrf.predict(X_test_meta, z_test, test_clusters)\n",
    "mse = mean_squared_error(y_test_meta, predictions)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved a relatively low MSE! However this result must be taken with a large grain of salt, as it is entirely dependent on each train/test split. A different split (modifying random_state), could give a much different result. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
