{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "pd.options.mode.chained_assignment = None\n",
    "# pickle_out = open(\"datasets_cleaned.pkl\",\"wb\")\n",
    "# pickle.dump(datasets1, pickle_out)                #useful for writing data\n",
    "# pickle_out.close()\n",
    "def load_obj(name ):                    \n",
    "    with open('' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['cellfreerna.csv','plasmaluminex.csv','serumluminex.csv','microbiome.csv','immunesystem.csv','metabolomics.csv','plasmasomalogic.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the cellfreerna dataset\n",
      "Successfully loaded the plasmaluminex dataset\n",
      "Successfully loaded the serumluminex dataset\n",
      "Successfully loaded the microbiome dataset\n",
      "Successfully loaded the immunesystem dataset\n",
      "Successfully loaded the metabolomics dataset\n",
      "Successfully loaded the plasmasomalogic dataset\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "for file in filenames:\n",
    "    data = pd.read_csv(file)\n",
    "    data.rename(columns={'Unnamed: 0':'PatientID'},inplace=True)\n",
    "    data = data.set_index('PatientID', drop=True)\n",
    "    if 'response' not in datasets.keys():\n",
    "        datasets['response'] = data.featureweeks\n",
    "    data = data.drop('featureweeks', axis=1)\n",
    "    datasets[file[0:file.find('.')]] = data\n",
    "    print('Successfully loaded the', file[0:file.find('.')], 'dataset' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A while after initially exploring the data I noticed that some datasets have different indices. The index is formatted as PTLG00X_1 through PTLG00X_4 for most datasets, where X represents the patient's id and the second number represents the trimester. However for other datasets it is formatted as PTLG00X_BL, PTLG00X_1 through PTLG00X_3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PTLG002_1', 'PTLG003_1', 'PTLG004_1', 'PTLG005_1', 'PTLG007_1',\n",
       "       'PTLG008_1', 'PTLG009_1', 'PTLG010_1', 'PTLG012_1', 'PTLG015_1',\n",
       "       'PTLG016_1', 'PTLG018_1', 'PTLG019_1', 'PTLG020_1', 'PTLG022_1',\n",
       "       'PTLG024_1', 'PTLG029_1', 'PTLG002_2', 'PTLG003_2', 'PTLG004_2',\n",
       "       'PTLG005_2', 'PTLG007_2', 'PTLG008_2', 'PTLG009_2', 'PTLG010_2',\n",
       "       'PTLG012_2', 'PTLG015_2', 'PTLG016_2', 'PTLG018_2', 'PTLG019_2',\n",
       "       'PTLG020_2', 'PTLG022_2', 'PTLG024_2', 'PTLG029_2', 'PTLG002_3',\n",
       "       'PTLG003_3', 'PTLG004_3', 'PTLG005_3', 'PTLG007_3', 'PTLG008_3',\n",
       "       'PTLG009_3', 'PTLG010_3', 'PTLG012_3', 'PTLG015_3', 'PTLG016_3',\n",
       "       'PTLG018_3', 'PTLG019_3', 'PTLG020_3', 'PTLG022_3', 'PTLG024_3',\n",
       "       'PTLG029_3', 'PTLG002_4', 'PTLG003_4', 'PTLG004_4', 'PTLG005_4',\n",
       "       'PTLG007_4', 'PTLG008_4', 'PTLG009_4', 'PTLG010_4', 'PTLG012_4',\n",
       "       'PTLG015_4', 'PTLG016_4', 'PTLG018_4', 'PTLG019_4', 'PTLG020_4',\n",
       "       'PTLG022_4', 'PTLG024_4', 'PTLG029_4'],\n",
       "      dtype='object', name='PatientID')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['cellfreerna'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PTLG002_BL', 'PTLG003_BL', 'PTLG004_BL', 'PTLG005_BL', 'PTLG007_BL',\n",
       "       'PTLG008_BL', 'PTLG009_BL', 'PTLG010_BL', 'PTLG012_BL', 'PTLG015_BL',\n",
       "       'PTLG016_BL', 'PTLG018_BL', 'PTLG019_BL', 'PTLG020_BL', 'PTLG022_BL',\n",
       "       'PTLG024_BL', 'PTLG029_BL', 'PTLG002_1', 'PTLG003_1', 'PTLG004_1',\n",
       "       'PTLG005_1', 'PTLG007_1', 'PTLG008_1', 'PTLG009_1', 'PTLG010_1',\n",
       "       'PTLG012_1', 'PTLG015_1', 'PTLG016_1', 'PTLG018_1', 'PTLG019_1',\n",
       "       'PTLG020_1', 'PTLG022_1', 'PTLG024_1', 'PTLG029_1', 'PTLG002_2',\n",
       "       'PTLG003_2', 'PTLG004_2', 'PTLG005_2', 'PTLG007_2', 'PTLG008_2',\n",
       "       'PTLG009_2', 'PTLG010_2', 'PTLG012_2', 'PTLG015_2', 'PTLG016_2',\n",
       "       'PTLG018_2', 'PTLG019_2', 'PTLG020_2', 'PTLG022_2', 'PTLG024_2',\n",
       "       'PTLG029_2', 'PTLG002_3', 'PTLG003_3', 'PTLG004_3', 'PTLG005_3',\n",
       "       'PTLG007_3', 'PTLG008_3', 'PTLG009_3', 'PTLG010_3', 'PTLG012_3',\n",
       "       'PTLG015_3', 'PTLG016_3', 'PTLG018_3', 'PTLG019_3', 'PTLG020_3',\n",
       "       'PTLG022_3', 'PTLG024_3', 'PTLG029_3'],\n",
       "      dtype='object', name='PatientID')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['immunesystem'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data was loaded with the author's function, I was rather confused. \n",
    "\n",
    "Upon further investigation it seems that there are some discrepancies between the label and the value of an R vector. Based on intuition, the values of the response, and the author's code I hesitantly conclude that each dataset is correctly ordered, and I will simply change the value of the indices to PTLG00X_1 through PTLG00X_4. In practice I would most likely contact the author as this could severely effect my results if incorrect. I make this change below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctIndex = datasets['cellfreerna'].index\n",
    "for name, data in datasets.items():\n",
    "    if data.index.any() != correctIndex.any():\n",
    "        data.index = correctIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the shapes for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response has dimensions (68,)\n",
      "cellfreerna has dimensions (68, 37101)\n",
      "plasmaluminex has dimensions (68, 62)\n",
      "serumluminex has dimensions (68, 62)\n",
      "microbiome has dimensions (68, 18548)\n",
      "immunesystem has dimensions (68, 534)\n",
      "metabolomics has dimensions (68, 3485)\n",
      "plasmasomalogic has dimensions (68, 1300)\n"
     ]
    }
   ],
   "source": [
    "for name, data in datasets.items():\n",
    "    print(name, 'has dimensions', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Data Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for duplicates in the rest of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDuplicates(data, name):\n",
    "    nrowsduplicated = sum(data.duplicated())\n",
    "    ncolsduplicated = sum(data.transpose().duplicated())\n",
    "    print('There are {} duplicate rows and {} duplicate columns in the {} dataset'.format(nrowsduplicated, ncolsduplicated, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 duplicate rows and 7184 duplicate columns in the cellfreerna dataset\n",
      "There are 0 duplicate rows and 0 duplicate columns in the plasmaluminex dataset\n",
      "There are 6 duplicate rows and 0 duplicate columns in the serumluminex dataset\n",
      "There are 29 duplicate rows and 16627 duplicate columns in the microbiome dataset\n",
      "There are 0 duplicate rows and 0 duplicate columns in the immunesystem dataset\n",
      "There are 0 duplicate rows and 0 duplicate columns in the metabolomics dataset\n",
      "There are 3 duplicate rows and 0 duplicate columns in the plasmasomalogic dataset\n"
     ]
    }
   ],
   "source": [
    "for name, data in datasets.items():\n",
    "    if name in ['response']:\n",
    "        continue\n",
    "    checkDuplicates(data, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am wary of removing duplicate rows due to my lack of domain expertise, though I will take note that nearly half of the rows in the microbiome dataset are duplicated. However I do believe it is safe to remove duplicate columns as I believe they do not provide any extra information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['cellfreerna'] = datasets['cellfreerna'].transpose().drop_duplicates().transpose()\n",
    "datasets['microbiome'] = datasets['microbiome'].transpose().drop_duplicates().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will check and remove extremely sparse features, as the data loading function written by the author only exclude completely sparse features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparseFeatures(data, threshold, lengthOnly = False, name = 'dataset'):\n",
    "    '''\n",
    "    data: pandas DataFrame object\n",
    "    threshold: percentage of zero values within a feature necessary to be qualified as sparse\n",
    "    Returns a nested list of sparse features and their respective zero counts \n",
    "    '''\n",
    "    zeroCounts = []\n",
    "    sparsefeatures = []\n",
    "    n = data.shape[0]\n",
    "    for column in data.columns:\n",
    "        count = sum(data[column] == 0)\n",
    "        zeroCounts.append([column,count])\n",
    "    for lists in zeroCounts:\n",
    "        if lists[1] > threshold*n:\n",
    "            sparsefeatures.append([lists[0], lists[1]])\n",
    "    if len(sparsefeatures) == 0:\n",
    "        print('There are no sparse features in {} dataset'.format(name))\n",
    "        return []\n",
    "    if lengthOnly:\n",
    "        return len(sparsefeatures)\n",
    "    else:\n",
    "        return sparsefeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cellfreerna dataset has 11718 sparse features. These will be removed.\n",
      "There are no sparse features in plasmaluminex dataset\n",
      "There are no sparse features in serumluminex dataset\n",
      "There are no sparse features in microbiome dataset\n",
      "There are no sparse features in immunesystem dataset\n",
      "There are no sparse features in metabolomics dataset\n",
      "There are no sparse features in plasmasomalogic dataset\n"
     ]
    }
   ],
   "source": [
    "for name, data in datasets.items():\n",
    "    if name in ['response']:\n",
    "        continue\n",
    "    sf = sparseFeatures(data, 0.8, name=name)\n",
    "    if len(sf) != 0:\n",
    "        print('{} dataset has {} sparse features. These will be removed.'.format(name, len(sf)))\n",
    "        datasets[name] = datasets[name].drop([x[0] for x in sf], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the cellfreerna dataset had nearly 12 thousand features each having more than 80% zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the variance of these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = datasets.pop('response', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The microbiome dataset has 703 features with a variance less than 0.00001\n",
      "The immunesystem dataset has 10 features with a variance less than 0.00001\n"
     ]
    }
   ],
   "source": [
    "selected = {}\n",
    "for name, data in datasets.items():\n",
    "    X = data\n",
    "    selector = VarianceThreshold()\n",
    "    selector.fit(X)\n",
    "    if sum(selector.variances_ <= 0.0001):\n",
    "        selected[name] = selector.variances_ <= 0.0001\n",
    "        print('The {} dataset has {} features with a variance less than 0.00001'.format(name,sum(selector.variances_ <= 0.0001)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With such a low variance, they most likely do not contribute much to the response. Let's drop those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['microbiome'] = datasets['microbiome'].loc[:,~selected['microbiome']]\n",
    "datasets['immunesystem'] = datasets['immunesystem'].loc[:,~selected['immunesystem']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection via Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets1 = copy.deepcopy(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in datasets:\n",
    "    datasets[data] = datasets[data].iloc[0:51,:]\n",
    "response = response[0:51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 features were selected for the cellfreerna dataset.\n",
      "14 features were selected for the plasmaluminex dataset.\n",
      "19 features were selected for the serumluminex dataset.\n",
      "55 features were selected for the microbiome dataset.\n",
      "57 features were selected for the immunesystem dataset.\n",
      "75 features were selected for the metabolomics dataset.\n",
      "40 features were selected for the plasmasomalogic dataset.\n"
     ]
    }
   ],
   "source": [
    "selectedFeatures = {}\n",
    "selectedFrom = {}\n",
    "for name, data in datasets.items():\n",
    "    X = data\n",
    "    y = response\n",
    "    params = {\n",
    "            \"n_estimators\" : [10], #,50,100],\n",
    "            \"max_features\" : [\"auto\"], #, \"log2\", \"sqrt\"], ## grid removed due to computational limitations\n",
    "            \"bootstrap\"    : [True] #, False]\n",
    "        }\n",
    "    \n",
    "    #LOPOCV\n",
    "    groups = np.array(list(range(1,18))*3)\n",
    "    logo = LeaveOneGroupOut()\n",
    "    cv = logo.split(X, y, groups=groups)\n",
    "    #Tuning Model \n",
    "    rf = RandomForestRegressor()\n",
    "    grid = GridSearchCV(rf, params, scoring='neg_mean_squared_error', cv=cv, iid=False)\n",
    "    grid.fit(X, y)\n",
    "    rf_tuned = grid.best_estimator_\n",
    "    selectedfrom = []\n",
    "    for feature, value in zip(X.columns, rf_tuned.feature_importances_):\n",
    "        selectedfrom.append((feature, value))\n",
    "    if name == 'cellfreerna':\n",
    "        sfm = SelectFromModel(rf_tuned, threshold=0.00009) \n",
    "    else:\n",
    "        sfm = SelectFromModel(rf_tuned, threshold='mean')\n",
    "    # Train the selector\n",
    "    sfm.fit(X, y)\n",
    "    selected = X.columns[sfm.get_support()]\n",
    "    selectedFeatures[name] = selected\n",
    "    selectedFrom[name] = selectedfrom\n",
    "    print('{} features were selected for the {} dataset.'.format(len(selected),name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in datasets:\n",
    "    datasets[name] = datasets[name][selectedFeatures[name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"datasets_selected.pkl\",\"wb\")\n",
    "pickle.dump(datasets, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"response.pkl\",\"wb\")\n",
    "pickle.dump(response, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
